---
title: "Furman Basketball Predictions"
author: "Matt Smith"
format: gfm
execute:
    warning: false
    message: false
    error: false
---

```{python}
import pandas as pd

furman = pd.read_csv("~/Downloads/Furman Basketball - Final.csv")

furman.head()
furman.tail()
```

```{python}
#| output: asis
furman.describe(include= 'all')
```

Research Question

What attributes best predict a Furman Basketball win, and do these predictors change once the team enters conference play?

To begin the analysis, I created a new binary outcome variable to make logistic regression possible. Specifically, I encoded game results such that Win = 1 and Loss = 0. This allows the model to estimate how different game attributes influence the probability of winning.

```{python}
furman['Win_Binary'] = (furman['Result'] == 'Win').astype(int)
furman
```

Next, I used logistic regression to identify which variables most strongly predict the likelihood of a Furman win.

Before fitting the model, I prepared the dataset by selecting and formatting the numeric variables. This step ensures that only appropriate predictors are included and that the logistic regression model runs correctly.

```{python}
import re
numeric_furman = furman.select_dtypes(include='number')
numeric_furman.columns = [re.sub('1', 'one', x) for x in numeric_furman.columns]
numeric_furman.columns = [re.sub('2', 'two', x) for x in numeric_furman.columns]
numeric_furman.columns = [re.sub('3', 'three', x) for x in numeric_furman.columns]
numeric_furman.columns = [re.sub('%', 'perc', x) for x in numeric_furman.columns]
numeric_furman.columns = [re.sub('\.', '_', x) for x in numeric_furman.columns]
```

Because many of the original game statistics were highly correlated, I created “difference” variables for each attribute. For every statistic reported for both Furman and its opponent, I calculated:

Difference = Furman Stat − Opponent Stat

This transformation preserves the original scale of each metric while reducing multicollinearity. It also makes the variables more interpretable in the logistic regression model, since each coefficient now represents how an advantage in a specific stat increases or decreases the probability of a Furman win.

```{python}
for col in numeric_furman.columns:
    if col.endswith("_Furman"):
        base = col.replace("_Furman", "")
        opp_col = base + "_Opponent"
        
        if opp_col in numeric_furman.columns:
            diff_name = "Diff_" + base
            numeric_furman[diff_name] = numeric_furman[col] - numeric_furman[opp_col]
```

Confirmation that the code above runs correct:

```{python}
numeric_furman['FG_Furman'][0]
```
```{python}
numeric_furman['FG_Opponent'][0]
```
```{python}
numeric_furman['Diff_FG'][0]
```

Next, I generated scatterplots to check for complete or quasi-complete separation in the data. Separation can occur when a predictor perfectly distinguishes wins from losses, which can cause instability or failure in logistic regression estimation. Visualizing each variable against the win/loss outcome helps identify any predictors that may create this issue before fitting the model.

```{python}
import matplotlib.pyplot as plt

x_vars = [
    'Diff_FTA', 'Diff_ORB', 'Diff_DRB', 'Diff_AST',
    'Diff_STL', 'Diff_BLK', 'Diff_TOV', 'Diff_PF'
]

for var in x_vars:
    plt.figure(figsize=(6,4))
    plt.scatter(numeric_furman[var], numeric_furman['Win_Binary'], alpha=0.6)
    plt.title(f'Win Binary vs {var}')
    plt.xlabel(var)
    plt.ylabel('Win_Binary')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.show()
```

The Diff_AST graph looks to be causing some seperating. I will remove that as well as others that caused seperation.

```{python}
predictors = [col for col in numeric_furman.columns if col.startswith("Diff_")]

remove_cols = [
    'Diff_FGperc', 'Diff_threePperc', 'Diff_twoPperc',
    'Diff_eFGperc', 'Diff_FTperc', 
    'Diff_TRB',
    'Diff_FGA',
    'Diff_threeP',
    'Diff_twoP',
    'Diff_FT',
    'Diff_FG', 
    'Diff_threePA',
    'Diff_twoPA',
    'Diff_AST'
]

predictors = [col for col in predictors if col not in remove_cols]
```

The scatterplot for Diff_AST showed evidence of separation, meaning that this variable almost perfectly distinguished wins from losses. To avoid instability in the logistic regression model, I removed Diff_AST along with any other variables that exhibited similar separation or multicollinearity issues.

```{python}
import statsmodels.formula.api as smf

model1 = smf.logit(
    formula = "Win_Binary ~ " + " + ".join(predictors), #needed AI to figure this one out
    data = numeric_furman
).fit()

model1.summary()
```

Assists and three pointers turned out to be too strong as predictors, creating separation in the logistic regression model. Because these statistics almost perfectly distinguish wins from losses, they need to be removed to ensure the model can be estimated properly. I keep this observation in the write up to show that certain variables, such as points scored, are directly tied to the game outcome and should be excluded from predictive modeling for interpretability and fairness.

After fitting the refined logistic regression model, Diff_FTA, Diff_DRB, Diff_BLK, and Diff_TOV emerged as statistically significant predictors of a Furman win. These metrics suggest that getting to the free throw line, controlling the defensive glass, protecting the rim, and limiting turnovers all have meaningful effects on game outcomes.

To complement the logistic regression results, I also trained a decision tree model. The goal of the tree is to identify which single attribute the model chooses to split on first. This highlights which individual variable, at a high level, is most predictive of a Furman victory.

Before I can run any models, I want to define my response variable and set up the predictors. I need to pull out the game result and remove any columns that should not be included in the model.

```{python}
y = furman["Result"] # Set explanatory variables
X = furman.drop(columns=["Result", "Win_Binary", "Tm", "Opp.1"]) # Set response variable
```

Next, I need to think about preprocessing. Since my dataset includes both numeric and categorical variables, I want a pipeline that handles each type properly. My plan is to identify the column types, build specific category mappings, and then apply one hot encoding to the categorical columns while imputing missing numeric values.

```{python}
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Identify column types
cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()

# Build explicit categories per categorical column
cats_map = {c: sorted(X[c].dropna().unique().tolist()) for c in cat_cols}
cat_cols_eff = [c for c, cats in cats_map.items() if len(cats) > 0]
ohe_categories = [cats_map[c] for c in cat_cols_eff]

# Preprocess: one-hot encode categoricals and impute missing values
preprocess = ColumnTransformer(
    transformers=[
        ("cat",
         OneHotEncoder(
             categories=ohe_categories,
             handle_unknown="ignore",
             sparse_output=False,
             drop=None  # keep both dummies for binary so NaN -> [0,0]
         ),
         cat_cols_eff),
        ("num",
         Pipeline([("imputer", SimpleImputer(strategy="median"))]),
         num_cols),
    ],
    remainder="drop",
)
```

Now that I know how my data will be preprocessed, I want to build the decision tree classifier. My thinking here is to keep the tree shallow so I can clearly see the strongest predictive split without the model overfitting.

```{python}
from sklearn.tree import DecisionTreeClassifier, plot_tree

tree_clf = DecisionTreeClassifier(random_state=123,
max_depth=3)
```

At this point, I need to actually transform the dataset using the preprocessing pipeline. I also need to encode the target variable so the model can interpret it correctly. Here is how I think about it and then implement it:

```{python}
from sklearn.preprocessing import LabelEncoder
X_trans = preprocess.fit_transform(X)

# Encode target to integers
le = LabelEncoder()
y_enc = le.fit_transform(y)                # e.g., {'no_help':0, 'sought_help':1}
class_names = le.classes_.tolist() 
```

Now that everything is prepared, I can fit the model.

```{python}
tree_clf.fit(X_trans, y_enc)
```

```{python}
import dtreeviz
viz_model = dtreeviz.model(
    tree_clf,
    X_train=X_trans,
    y_train=y_enc,
    feature_names=preprocess.get_feature_names_out(),
    target_name="Result",
    class_names=class_names  # keep stable order
)

v = viz_model.view(fontname="DejaVu Sans")
v
```

When I look at the decision tree, shooting percentages clearly stand out as the strongest indicators of a Furman win. The model chooses opponent field goal percentage as the very first split, which tells me that limiting how well the other team shoots is the most important high-level factor in predicting success.

# SoCon
While looking at all the data together is helpful, I need to recognize the reality of mid-major basketball. It is extremely rare to reach the NCAA Tournament without winning the conference title. Because Furman plays in the SoCon, the non-conference schedule functions more as a testing ground to learn about the team’s strengths and weaknesses. What truly matters for Furman’s postseason hopes is the SoCon regular season and, especially, the conference tournament.

Teams in the SoCon also scout each other during the non-conference slate and will tailor their game plans once they have film on you. Because of that, it can even be advantageous to keep some of your best offensive and defensive sets hidden until conference play begins. If this is the case, the most important variables for predicting wins may differ between non-conference and conference games. This is exactly why I want to separate the two and analyze whether the key predictors change once Furman enters SoCon play.

```{python}
socon = furman[furman['Type'].isin(['REG (Conf)', 'CTOURN'])]
socon
```

I am now going to rerun both the logistic regression and the decision tree, but this time I will focus only on SoCon games. My goal is to see whether the most important predictors of a Furman win change once conference play starts. I will use the same logic and code structure that I used above, just applied to the SoCon subset of the data.

```{python}
import re
numeric_socon = socon.select_dtypes(include='number')
numeric_socon.columns = [re.sub('1', 'one', x) for x in numeric_socon.columns]
numeric_socon.columns = [re.sub('2', 'two', x) for x in numeric_socon.columns]
numeric_socon.columns = [re.sub('3', 'three', x) for x in numeric_socon.columns]
numeric_socon.columns = [re.sub('%', 'perc', x) for x in numeric_socon.columns]
numeric_socon.columns = [re.sub('\.', '_', x) for x in numeric_socon.columns]
```


```{python}
for col in numeric_socon.columns:
    if col.endswith("_Furman"):
        base = col.replace("_Furman", "")
        opp_col = base + "_Opponent"
        
        if opp_col in numeric_socon.columns:
            diff_name = "Diff_" + base
            numeric_socon[diff_name] = numeric_socon[col] - numeric_socon[opp_col]
```

Confirmation that code above works:

```{python}
numeric_socon['FG_Furman'][13]
```


```{python}
numeric_socon['FG_Opponent'][13]
```


```{python}
numeric_socon['Diff_FG'][13]
```

Now recheck for seperation:

```{python}
import matplotlib.pyplot as plt

x_vars = [
    'Diff_FTA', 'Diff_ORB', 'Diff_DRB', 'Diff_AST',
    'Diff_STL', 'Diff_BLK', 'Diff_TOV', 'Diff_PF'
]

for var in x_vars:
    plt.figure(figsize=(6,4))
    plt.scatter(numeric_socon[var], numeric_socon['Win_Binary'], alpha=0.6)
    plt.title(f'Win Binary vs {var}')
    plt.xlabel(var)
    plt.ylabel('Win_Binary')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.show()
```

Diff_AST again looks like it needs to be removed.

Now run logistic regression:

```{python}
predictors = [col for col in numeric_socon.columns if col.startswith("Diff_")]

remove_cols = [
    'Diff_FGperc', 'Diff_threePperc', 'Diff_twoPperc',
    'Diff_eFGperc', 'Diff_FTperc', 
    'Diff_TRB',
    'Diff_FGA',
    'Diff_threeP',
    'Diff_twoP',
    'Diff_FT',
    'Diff_FG', 
    'Diff_threePA',
    'Diff_twoPA',
    'Diff_AST'
]

predictors = [col for col in predictors if col not in remove_cols]
```


```{python}
model2 = smf.logit(
    formula = "Win_Binary ~ " + " + ".join(predictors), #needed AI to figure this one out
    data = numeric_socon
).fit()

model2.summary()
```

From the logistic regression on only SoCon games, the statistically significant variables are Diff_FTA, Diff_DRB, and Diff_TOV. These results tell me that getting to the free throw line, controlling the defensive boards, and limiting turnovers continue to play an important role within conference play. Assists and three pointers again were too predictive for this.

Now I want to run the decision tree on the SoCon subset to see what single variable it chooses as the top splitter. This will help me understand which attribute is the most important high-level indicator of a Furman win specifically during conference games.


```{python}
y = socon["Result"] # Set explanatory variables
X = socon.drop(columns=["Result", "Win_Binary", "Tm", "Opp.1"]) # Set response
```


```{python}
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Identify column types
cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()

# Build explicit categories per categorical column
cats_map = {c: sorted(X[c].dropna().unique().tolist()) for c in cat_cols}
cat_cols_eff = [c for c, cats in cats_map.items() if len(cats) > 0]
ohe_categories = [cats_map[c] for c in cat_cols_eff]

# Preprocess: one-hot encode categoricals and impute missing values
preprocess = ColumnTransformer(
    transformers=[
        ("cat",
         OneHotEncoder(
             categories=ohe_categories,
             handle_unknown="ignore",
             sparse_output=False,
             drop=None  # keep both dummies for binary so NaN -> [0,0]
         ),
         cat_cols_eff),
        ("num",
         Pipeline([("imputer", SimpleImputer(strategy="median"))]),
         num_cols),
    ],
    remainder="drop",
)
```

```{python}
from sklearn.tree import DecisionTreeClassifier, plot_tree

tree_clf = DecisionTreeClassifier(random_state=123,
max_depth=3)
```

```{python}
from sklearn.preprocessing import LabelEncoder
X_trans = preprocess.fit_transform(X)

# Encode target to integers
le = LabelEncoder()
y_enc = le.fit_transform(y)                # e.g., {'no_help':0, 'sought_help':1}
class_names = le.classes_.tolist() 
```

```{python}
tree_clf.fit(X_trans, y_enc)
```

```{python}
import dtreeviz
viz_model = dtreeviz.model(
    tree_clf,
    X_train=X_trans,
    y_train=y_enc,
    feature_names=preprocess.get_feature_names_out(),
    target_name="Result",
    class_names=class_names  # keep stable order
)

v = viz_model.view(fontname="DejaVu Sans")
v
```

From the decision tree on SoCon games, a high eFG% comes out as the most important variable. This makes sense, because eFG% accounts for the added value of three pointers and reflects the overall efficiency of both two and three point shooting. Given that the SoCon is known as a high-octane, three-point-heavy league, it is logical that shooting efficiency would play an even larger role in predicting wins during conference play.

When I analyzed the full season data, several variables stood out as meaningful predictors of a Furman win. After removing statistics that caused separation in the logistic regression model, such as assists and three-point makes, I found that Diff_FTA, Diff_DRB, Diff_BLK, and Diff_TOV were statistically significant. These results suggest that winning the free throw battle, controlling the defensive glass, protecting the rim, and taking care of the ball are all important indicators of success. The decision tree reinforced the importance of shooting efficiency, as opponent field goal percentage emerged as the top splitter when looking at all games. This shows that limiting the opponent’s shooting success is the single strongest high-level factor in predicting a Furman victory across the full season.

However, when I narrowed the analysis to conference play, some patterns shifted. Within SoCon games, Diff_FTA, Diff_DRB, and Diff_TOV remained significant in the logistic regression, showing that these fundamentals still matter once the games become more important. But the decision tree revealed a different top predictor: effective field goal percentage. This makes sense given the SoCon’s reputation as a three-point-heavy, high-pace league, where shooting efficiency has an even greater impact on game outcomes. Overall, the analysis shows that while fundamentals like rebounding, free throws, and ball security are consistently important, the value of shooting efficiency becomes especially prominent once Furman enters conference play.